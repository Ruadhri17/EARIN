{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# import gym\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions import Beta\n",
    "# from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "# # from utils import DrawLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.7.5 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/usr/bin/python3.7 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING  130 / 144\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Trying to log data to tensorboard but tensorboard is not installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-742c1c6081e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LEARNING \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timesteps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0ml_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{0:0.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         total_timesteps, callback = self._setup_learn(\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         )\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_setup_learn\u001b[0;34m(self, total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, log_path, reset_num_timesteps, tb_log_name)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# Configure logger's outputs if no logger was passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_custom_logger\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m# Create eval callback if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/common/utils.py\u001b[0m in \u001b[0;36mconfigure_logger\u001b[0;34m(verbose, tensorboard_log, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensorboard_log\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mSummaryWriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trying to log data to tensorboard but tensorboard is not installed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensorboard_log\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mSummaryWriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Trying to log data to tensorboard but tensorboard is not installed."
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.vec_env import VecFrameStack\n",
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "# import csv\n",
    "# import timeit\n",
    "\n",
    "# #Save logs to file\n",
    "# log_path = ('Logs')\n",
    "# filename = \"Logs/Results8.csv\"\n",
    "\n",
    "# #Variable for highscore storing\n",
    "# highscore = -10000\n",
    "\n",
    "# #Create an environment\n",
    "# env = gym.make(\"CarRacing-v1\")\n",
    "\n",
    "# #Variables for grid search\n",
    "# param_grid = {'l_rate': [0.0001, 0.001, 0.01, 0.1], 'policy' : [\"MlpPolicy\", \"CnnPolicy\"], 'n_steps': [1024, 2048, 4096], 'n_epochs': [5, 10, 20], 'timesteps': [1000, 10000]}\n",
    "\n",
    "# grid = ParameterGrid(param_grid)\n",
    "# iter = 0\n",
    "\n",
    "# #Grid seatch on model\n",
    "# for params in grid:\n",
    "\n",
    "# \titer += 1\t\n",
    "# \tif iter<130:\n",
    "# \t\tcontinue\n",
    "\t\n",
    "# \t#Prepare model\n",
    "# \tmodel = PPO(params['policy'], env, verbose=0, learning_rate=params['l_rate'],tensorboard_log=log_path, seed=2137, n_steps=params['n_steps'], n_epochs=params['n_epochs'])\n",
    "\n",
    "# \t#Perform learning procedure\n",
    "# \tprint(\"LEARNING \", iter, \"/\", len(grid))\n",
    "# \tstart = timeit.default_timer()\n",
    "# \tmodel.learn(total_timesteps=params['timesteps'])\n",
    "# \tstop = timeit.default_timer()\n",
    "# \tl_time = \"{0:0.3f}\".format(stop - start)\n",
    "\n",
    "# \t#Evaluate learning outcomes\n",
    "# \tprint(\"EVALUATING \", iter, \"/\", len(grid))\n",
    "# \t#Returns (Mean reward, Standard deviation)\n",
    "# \t#Change render to true to see results\n",
    "# \tmean, std = evaluate_policy(model, env, n_eval_episodes=3, render=False)\n",
    "# \tprint(\"Result: \", mean)\n",
    "\t\n",
    "# \t#Save results\n",
    "# \twith open(filename,\"a\") as my_csv:\n",
    "# \t\tcsvWriter = csv.writer(my_csv,delimiter=';')\n",
    "# \t\tcsvWriter.writerow([params['l_rate'], params['policy'], params['n_steps'], params['n_epochs'], params['timesteps'], l_time, mean])\n",
    "\t\n",
    "# \t#Check if it is currently the best model\n",
    "# \tif mean > highscore:\n",
    "# \t\thighscore = mean\n",
    "# \t\tprint(\"New highscore: \", highscore)\n",
    "# \t\t#Save the best model to file in a .zip format\n",
    "# \t\tprint(\"SAVING TO FILE\")\n",
    "# \t\tmodel.save('Logs/Model')\n",
    "\n",
    "# ''' \n",
    "# #THIS IS HOW LOADING MODEL FROM FILE WOULD LOOK LIKE\n",
    "# model.load('Logs/Model')\n",
    "# '''\n",
    "\n",
    "# #Close environment\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PPO2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-20d305134519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# from stable_baselines3.common.policies import CnnPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PPO2'"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "# # from gym.envs.box2d import CarRacing\n",
    "\n",
    "# # from stable_baselines3.common.policies import CnnPolicy\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# from stable_baselines3 import PPO\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     env = lambda :  CarRacing(\n",
    "#         grayscale=1,\n",
    "#         show_info_panel=0,\n",
    "#         discretize_actions=\"hard\",\n",
    "#         frames_per_state=4,\n",
    "#         num_lanes=1,\n",
    "#         num_tracks=1,\n",
    "#         )\n",
    "\n",
    "#     #env = getattr(environments, env)\n",
    "#     env = DummyVecEnv([env])\n",
    "\n",
    "#     model = PPO2.load('car_racing_weights.pkl')\n",
    "\n",
    "#     model.set_env(env)\n",
    "\n",
    "#     obs = env.reset()\n",
    "#     while True:\n",
    "#         action, _states = model.predict(obs)\n",
    "#         obs, rewards, dones, info = env.step(action)\n",
    "#         env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 10\n",
    "# img_stack = 4\n",
    "# action_repeat = 10\n",
    "# render = True\n",
    "# gamma = 0.1\n",
    "# log_interval = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"CarRacing-v1\"\n",
    "# env = gym.make(env_name)\n",
    "# print(\"Observation space:\", env.observation_space)\n",
    "# print(\"Action space:\", env.action_space)\n",
    "# transition = np.dtype([('s', np.float64, (img_stack, 96, 96)), ('a', np.float64, (3,)), ('a_logp', np.float64),\n",
    "#                        ('r', np.float64), ('s_', np.float64, (img_stack, 96, 96))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Env():\n",
    "#     \"\"\"\n",
    "#     Environment wrapper for CarRacing \n",
    "#     \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.env = gym.make('CarRacing-v1')\n",
    "#         self.env.seed(seed)\n",
    "#         self.reward_threshold = self.env.spec.reward_threshold\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.counter = 0\n",
    "#         self.av_r = self.reward_memory()\n",
    "\n",
    "#         self.die = False\n",
    "#         img_rgb = self.env.reset()\n",
    "#         img_gray = self.rgb2gray(img_rgb)\n",
    "#         self.stack = [img_gray] * img_stack  # four frames for decision\n",
    "#         return np.array(self.stack)\n",
    "\n",
    "#     def step(self, action):\n",
    "#         total_reward = 0\n",
    "#         for i in range(action_repeat):\n",
    "#             img_rgb, reward, die, _ = self.env.step(action)\n",
    "#             # don't penalize \"die state\"\n",
    "#             if die:\n",
    "#                 reward += 100\n",
    "#             # green penalty\n",
    "#             if np.mean(img_rgb[:, :, 1]) > 185.0:\n",
    "#                 reward -= 0.05\n",
    "#             total_reward += reward\n",
    "#             # if no reward recently, end the episode\n",
    "#             done = True if self.av_r(reward) <= -0.1 else False\n",
    "#             if done or die:\n",
    "#                 break\n",
    "#         img_gray = self.rgb2gray(img_rgb)\n",
    "#         self.stack.pop(0)\n",
    "#         self.stack.append(img_gray)\n",
    "#         assert len(self.stack) == img_stack\n",
    "#         return np.array(self.stack), total_reward, done, die\n",
    "\n",
    "#     def render(self, *arg):\n",
    "#         self.env.render(*arg)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def rgb2gray(rgb, norm=True):\n",
    "#         # rgb image -> gray [0, 1]\n",
    "#         gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
    "#         if norm:\n",
    "#             # normalize\n",
    "#             gray = gray / 128. - 1.\n",
    "#         return gray\n",
    "\n",
    "#     @staticmethod\n",
    "#     def reward_memory():\n",
    "#         # record reward for last 100 steps\n",
    "#         count = 0\n",
    "#         length = 100\n",
    "#         history = np.zeros(length)\n",
    "\n",
    "#         def memory(reward):\n",
    "#             nonlocal count\n",
    "#             history[count] = reward\n",
    "#             count = (count + 1) % length\n",
    "#             return np.mean(history)\n",
    "\n",
    "#         return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Actor-Critic Network for PPO\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.cnn_base = nn.Sequential(  # input shape (4, 96, 96)\n",
    "#             nn.Conv2d(img_stack, 8, kernel_size=4, stride=2),\n",
    "#             nn.ReLU(),  # activation\n",
    "#             nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 47, 47)\n",
    "#             nn.ReLU(),  # activation\n",
    "#             nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 23, 23)\n",
    "#             nn.ReLU(),  # activation\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 11, 11)\n",
    "#             nn.ReLU(),  # activation\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 5, 5)\n",
    "#             nn.ReLU(),  # activation\n",
    "#             nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 3, 3)\n",
    "#             nn.ReLU(),  # activation\n",
    "#         )  # output shape (256, 1, 1)\n",
    "#         self.v = nn.Sequential(nn.Linear(256, 100), nn.ReLU(), nn.Linear(100, 1))\n",
    "#         self.fc = nn.Sequential(nn.Linear(256, 100), nn.ReLU())\n",
    "#         self.alpha_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "#         self.beta_head = nn.Sequential(nn.Linear(100, 3), nn.Softplus())\n",
    "#         self.apply(self._weights_init)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _weights_init(m):\n",
    "#         if isinstance(m, nn.Conv2d):\n",
    "#             nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "#             nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.cnn_base(x)\n",
    "#         x = x.view(-1, 256)\n",
    "#         v = self.v(x)\n",
    "#         x = self.fc(x)\n",
    "#         alpha = self.alpha_head(x) + 1\n",
    "#         beta = self.beta_head(x) + 1\n",
    "\n",
    "#         return (alpha, beta), v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Agent():\n",
    "#     \"\"\"\n",
    "#     Agent for training\n",
    "#     \"\"\"\n",
    "#     max_grad_norm = 0.5\n",
    "#     clip_param = 0.1  # epsilon in clipped loss\n",
    "#     ppo_epoch = 10\n",
    "#     buffer_capacity, batch_size = 2000, 128\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.training_step = 0\n",
    "#         self.net = Net().double().to(device)\n",
    "#         self.buffer = np.empty(self.buffer_capacity, dtype=transition)\n",
    "#         self.counter = 0\n",
    "\n",
    "#         self.optimizer = optim.Adam(self.net.parameters(), lr=1e-3)\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         state = torch.from_numpy(state).double().to(device).unsqueeze(0)\n",
    "#         with torch.no_grad():\n",
    "#             alpha, beta = self.net(state)[0]\n",
    "#         dist = Beta(alpha, beta)\n",
    "#         action = dist.sample()\n",
    "#         a_logp = dist.log_prob(action).sum(dim=1)\n",
    "\n",
    "#         action = action.squeeze().cpu().numpy()\n",
    "#         a_logp = a_logp.item()\n",
    "#         return action, a_logp\n",
    "\n",
    "#     # def save_param(self):\n",
    "#     #     torch.save(self.net.state_dict(), 'param/ppo_net_params.pkl')\n",
    "\n",
    "#     def store(self, transition):\n",
    "#         self.buffer[self.counter] = transition\n",
    "#         self.counter += 1\n",
    "#         if self.counter == self.buffer_capacity:\n",
    "#             self.counter = 0\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "\n",
    "#     def update(self):\n",
    "#         self.training_step += 1\n",
    "\n",
    "#         s = torch.tensor(self.buffer['s'], dtype=torch.double).to(device)\n",
    "#         a = torch.tensor(self.buffer['a'], dtype=torch.double).to(device)\n",
    "#         r = torch.tensor(self.buffer['r'], dtype=torch.double).to(device).view(-1, 1)\n",
    "#         s_ = torch.tensor(self.buffer['s_'], dtype=torch.double).to(device)\n",
    "\n",
    "#         old_a_logp = torch.tensor(self.buffer['a_logp'], dtype=torch.double).to(device).view(-1, 1)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             target_v = r + gamma * self.net(s_)[1]\n",
    "#             adv = target_v - self.net(s)[1]\n",
    "#             # adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "#         for _ in range(self.ppo_epoch):\n",
    "#             for index in BatchSampler(SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, False):\n",
    "\n",
    "#                 alpha, beta = self.net(s[index])[0]\n",
    "#                 dist = Beta(alpha, beta)\n",
    "#                 a_logp = dist.log_prob(a[index]).sum(dim=1, keepdim=True)\n",
    "#                 ratio = torch.exp(a_logp - old_a_logp[index])\n",
    "\n",
    "#                 surr1 = ratio * adv[index]\n",
    "#                 surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * adv[index]\n",
    "#                 action_loss = -torch.min(surr1, surr2).mean()\n",
    "#                 value_loss = F.smooth_l1_loss(self.net(s[index])[1], target_v[index])\n",
    "#                 loss = action_loss + 2. * value_loss\n",
    "\n",
    "#                 self.optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 # nn.utils.clip_grad_norm_(self.net.parameters(), self.max_grad_norm)\n",
    "#                 self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mieszko/.local/lib/python3.6/site-packages/gym/utils/env_checker.py:201: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  \"We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) \"\n",
      "/home/mieszko/.local/lib/python3.6/site-packages/gym/core.py:201: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
      "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0\tLast score: -20.23\tMoving average score: -0.20\n",
      "Ep 10\tLast score: 18.58\tMoving average score: 1.63\n",
      "Ep 20\tLast score: -18.06\tMoving average score: 2.76\n",
      "updating\n",
      "Ep 30\tLast score: -18.01\tMoving average score: 3.98\n",
      "Ep 40\tLast score: 98.04\tMoving average score: 3.92\n",
      "updating\n",
      "Ep 50\tLast score: -17.96\tMoving average score: 3.67\n",
      "Ep 60\tLast score: -17.98\tMoving average score: 4.45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5f2a50bfec57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1b5247a20615>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# if no reward recently, end the episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mav_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdie\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1b5247a20615>\u001b[0m in \u001b[0;36mmemory\u001b[0;34m(reward)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3372\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3373\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_float16_result\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_float16_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# agent = Agent()\n",
    "# env = Env()\n",
    "\n",
    "# training_records = []\n",
    "# running_score = 0\n",
    "# state = env.reset()\n",
    "# for i_ep in range(100000):\n",
    "#     score = 0\n",
    "#     state = env.reset()\n",
    "\n",
    "#     for t in range(1000):\n",
    "#         action, a_logp = agent.select_action(state)\n",
    "#         state_, reward, done, die = env.step(action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "#         if render:\n",
    "#             env.render()\n",
    "#         if agent.store((state, action, a_logp, reward, state_)):\n",
    "#             print('updating')\n",
    "#             agent.update()\n",
    "#         score += reward\n",
    "#         state = state_\n",
    "#         if done or die:\n",
    "#             break\n",
    "#     running_score = running_score * 0.99 + score * 0.01\n",
    "\n",
    "#     if i_ep % log_interval == 0:\n",
    "#         # if args.vis:\n",
    "#         #     draw_reward(xdata=i_ep, ydata=running_score)\n",
    "#         print('Ep {}\\tLast score: {:.2f}\\tMoving average score: {:.2f}'.format(i_ep, score, running_score))\n",
    "#         # agent.save_param()\n",
    "#     if running_score > env.reward_threshold:\n",
    "#         print(\"Solved! Running reward is now {} and the last episode runs to {}!\".format(running_score, score))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# for _ in range(100):\n",
    "#     env.render()\n",
    "#     env.step(env.action_space.sample()) # take a random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Env' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-1baceacf4cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Env' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "# env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd69f43f58546b570e94fd7eba7b65e6bcc7a5bbc4eab0408017d18902915d69"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
